{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Depth First Search(DFS)\n",
    " \n",
    "DFS is a greedy search algorithm. This algorithm does a search from a root node and goes down to the further end of the first child node it finds. It then steps back from the furthest node after it reaches the end and goes to the first sibling it finds and then goes to its furthest node. This way it will cover all nodes by navigating to the further leaf in each branch before  going to the next branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2) Breadth First Search(BFS)\n",
    " \n",
    "BFS is also a greedy search algorithm. This algorithm searches each of its sibling branches from the node root before going onto the children of the branches. This continues until it runs all the way till it reaches all the leaves. This is generally better used as we rarely want tot travel to the end of a tree to find a solution to a problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3) Uniform Cost Search\n",
    "\n",
    "UCS’s core principal is to utilize the selection of a weighted node which has the lowest cost. Each search session the priority is to explore the node which weighs the least before moving onto the next sibling. The data structure generally used for this algorithm is a priority queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4) Greedy Search\n",
    "\n",
    "A greedy search, much like its name implies is a simple a straightforward algorithm which looks for a given node. This decision is solely based on a simple principle of finding the best node in a localized setting (such as dfs or bfs). However, this is rarely a useful algorithm outside of learning environment as it is so localized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5) A* Search\n",
    "\n",
    "A* search is considered the first search algorithm which is generally considered fully useful. This is because the algorithm utilizes the concept of a greedy algorithm by searching for the local max. However it also searches the heuristics, which are calculated by a general distance to the goal. By summing the heuristic and local max, we get the A* search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6) Dijkstra's algorithm\n",
    "\n",
    "This is very famous algorithm which is generally used in conjunction with the Minimum Spanning Tree problem. This algorithm is used to find the shortest possible distance between a start and goal state. This is done by ‘discovering’ the distance for each node form the nod, then trating the dound nodes as a single node from which all out going edges are evaluated. This way if a new path is found which is more optimal, it is then updated to be the new shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7) Kruskal's Algorithm\n",
    " \n",
    "Kruskal’s algorithm is also a MST problem. However the way this algorithm works is that it has a working knowledge of all edge weights. Each time the edges are searched, starting from the start node, the edges found are added in ascending order. In this manner the minimum edges will be found to find the shortest path from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8) Prim's Algorithm\n",
    " \n",
    "Prim’s algorithm is the third MST conjunctive problem. The difference in behavior with the previous 2 MST conjunctive problems is that the chosen edges are a group of edges which include all vertices where the sum of weights and edges will be the most minimal. In doing so the weight is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9) Minimax Algorithm\n",
    " \n",
    "Minimax algorithms are algorithms which calculate decisions which are the best for yourself (agent 1) and the best for your opponent (agent2). This generally leads to a tree where agent1’s options are best case scenario, but the children moves by agent2 are the worst possible scenario for agent1. This cycle is continued down till base cases are reached and the decision with the highest value is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10) Expectimax Algorithm\n",
    " \n",
    "Expectimax builds off of the Minmax algorithm. Expectimax is a variation of the min max algorithm which also accounts for random chance moves. These chance moves can include rolls of dice, or acts of nature such as sheer randomness. Expectimax is generally better suited for real world games and scenarios as it can reflect scenarios which are not suited for determined decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11) Markov Decision Process\n",
    "\n",
    "Generally, MDP is defined using set of states, action and transition function, reward function, start state and terminal state. We can solve the problem using expectimax search. Here we require optimal plan to travel from stat to goal where policy gives action for each state and optimal policy maximizes the utilities. Explicit policy will define the reflex agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12) Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is any type of algorithm which utilizes the concept of using an output to an algorithm as a factor for encouraging or discouraging the algorithm’s behavior. In doing so the algorithm will then ‘adjust’ itself to better suit the problem. If done properly, Reinforcement algorithms will learn to suit themselves to a dataset and become optimized. They are the first type of algorithm which adjusts itself to suit the problem. A popular contemporary example of such algorithms are neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13) Q Learning\n",
    "\n",
    "Q Learning works on a simple premise, it combines the greedy nature of greedy algorithm and adds it to a future analysis. In this case the sum is the result of the greedy reward and future rewards. The Q function itself takes a state and returns and action as parameters and returns a reward. The policy function takes a state and produces an action. As the algorithm learns, the policy and Q functions will be updated to retrieve optimal results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
